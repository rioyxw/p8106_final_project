---
title: "Models"
author: Bin Yang
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)

library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(gbm)
```

#### Classification trees   
```{r classification tree: CART}
set.seed(1)  

rpart.fit = train(letter_grade ~ .,
                  student_df,
                  subset = rowTrain,
                  method = "rpart",
                  tuneGrid = data.frame(cp = exp(seq(-5, -2, len = 50))),
                  trControl = ctrl,
                  metric  = "Accuracy")
ggplot(rpart.fit, highlight = TRUE)

# rpart.plot(rpart.fit$finalModel)
# Best tuning parameter:  
rpart.fit$bestTune
```

```{r classification tree: CIT}  
set.seed(1)  

ctree.fit = train(letter_grade ~ .,
                  student_df,
                  subset = rowTrain,
                  method = "ctree",
                  tuneGrid = data.frame(mincriterion = 1 - exp(seq(-5, -3, length =50))),
                  metric = "Accuracy",
                  trControl = ctrl) 
ggplot(ctree.fit, highlight = TRUE)

# plot(ctree.fit$finalModel)
# Best tuning parameter:  
ctree.fit$bestTune
```

#### Random Forest   
```{r random forest,cache=TRUE}
rf.grid = expand.grid(mtry = 1:20,
                      splitrule = "gini",
                      min.node.size = seq(from = 2, to = 10, by = 2))  

set.seed(1)  

rf.fit = train(letter_grade ~ .,
               student_df,
               subset = rowTrain,
               method = "ranger", 
               tuneGrid = rf.grid,
               metric = "Accuracy",
               trControl = ctrl)
ggplot(rf.fit, highlight = TRUE)

# Best tuning parameter
rf.fit$bestTune
```

#### Adaboost   
```{r adaboost,cache=TRUE}
gbmA.grid = expand.grid(n.trees = c(1000, 2000, 3000),
                        interaction.depth = 1:3,
                        shrinkage = c(0.001, 0.003, 0.005),
                        n.minobsinnode = 1)

set.seed(1) 

gbmA.fit = train(letter_grade ~ .,
                 student_df,
                 subset = rowTrain,
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "Accuracy",
                 verbose = FALSE)

ggplot(gbmA.fit, highlight = TRUE)
```


#### Suppport vector classifier and SVM   
```{r support vector classifier,cache=TRUE}
set.seed(1)  
svml.fit = train(letter_grade ~ .,
                 data = student_df[rowTrain,],
                 method = "svmLinear",
                 tuneGrid = data.frame(C = exp(seq(-4,2,len=20))),
                 trControl = ctrl)

plot(svml.fit, highlight = TRUE, xTrans = log)
svml.fit$bestTune
```

```{r svm,cache=TRUE}
svmr.grid = expand.grid(C = exp(seq(-6, 1, len=10)),
                        sigma = exp(seq(-8, 0, len=10)))

set.seed(1) 
svmr.fit = train(letter_grade ~ ., student_df,
                 subset = rowTrain, 
                 method = "svmRadialSigma",
                 preProcess = c("center", "scale"),
                 tuneGrid = svmr.grid,
                 trControl = ctrl)

plot(svmr.fit, highlight = TRUE) 
svmr.fit$bestTune
```   

